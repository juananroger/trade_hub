{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db094711",
   "metadata": {},
   "source": [
    "# Part III â€“ Risk Evaluation and Consistency Testing\n",
    "This section addresses practical risk validation after model signals are generated and trades are submitted. The focus is on ensuring the system's execution remains aligned with expectations and does not drift materially from the intended portfolio state. While the proposed checklist includes infrastructure-heavy elements suited for production environments, we will focus on those tasks that are feasible and directly relevant to our current live deployment pipeline.\n",
    "\n",
    "## 11. Cross-checking performance between Zipline and IB \n",
    "To evaluate whether the strategy behaves as expected in a live environment, we compare daily and cumulative performance between the model and what was actually executed in the Interactive Brokers account. This includes:\n",
    "\n",
    "We run the full Zipline Reloaded backtest each day from a fixed start date (2025-08-15) up to the current session(2025-08-18), using updated EOD prices. From this, we extract the modelâ€™s NAV and daily returns. Separately, we extract NAV and realized returns from the IB account, based on confirmed trades and reported cash flows.\n",
    "\n",
    "From both sources, we compute daily return drift (the percent difference between model and live return) and cumulative NAV drift (the divergence in total equity over time). This allows us to catch mismatches due to missed orders, slippage, or rounding effects.\n",
    "\n",
    "We also compare these results at the individual contract level. For each futures symbol in the portfolio, we track return and NAV drift independently. This helps pinpoint where the largest deviations are coming fromâ€”whether from volatility in a single contract, execution errors, or incorrect sizing.\n",
    "\n",
    "Alongside drift analysis, we generate a performance report that includes key metrics over the tracking period. These include annualized return, cumulative return, volatility, and drawdowns, as well as Sharpe, Sortino, Calmar, and Omega ratios. Additional descriptive stats like skew, kurtosis, tail ratio, and daily VaR are also reported. Finally, alpha and beta to a reference index are included to benchmark relative risk-adjusted performance.\n",
    "\n",
    "This output serves as a practical diagnostic tool because it ensures that live execution is tracking the model closely enough to maintain confidence in the system, and it highlights when and where manual review is required.\n",
    "\n",
    "### 11.1. Live NAV Logging from Interactive Brokers\n",
    "\n",
    "To verify that the strategy behaves as expected in live deployment, we log the daily Net Asset Value (NAV) directly from the Interactive Brokers account. The aim is to compare this value with the NAV produced by the model each day, allowing us to detect divergence due to slippage, fills, fees, or logic mismatch.\n",
    "\n",
    "Each day after market close, we query two pieces of information from the IB account:\n",
    "\n",
    "1. NetLiquidation in EUR â€“ the total account value in EUR as the IB account is created in Europe and we cannot choose the base currency.\n",
    "\n",
    "2. EUR.USD FX Rate â€“ to convert NAV into USD, matching the model's denomination.\n",
    "\n",
    "Once both values are retrieved, we compute the USD NAV and log it to a CSV file (`ib_nav_log.csv`) along with the timestamp and the FX rate used. If thereâ€™s a previous NAV entry, we also calculate the daily percentage change (`drift`). This lets us compare daily returns from the broker with what the model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b003412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from ibapi.client import EClient\n",
    "from ibapi.wrapper import EWrapper\n",
    "from ibapi.contract import Contract\n",
    "from ibapi.common import BarData\n",
    "import os\n",
    "\n",
    "class IBapi(EWrapper, EClient):\n",
    "    def __init__(self):\n",
    "        EClient.__init__(self, self)\n",
    "        self.net_liquidation = None\n",
    "        self.fx_rate = None\n",
    "        self.received_nav = threading.Event()\n",
    "        self.received_fx = threading.Event()\n",
    "\n",
    "    def accountSummary(self, reqId, account, tag, value, currency):\n",
    "        if tag == \"NetLiquidation\" and currency == \"EUR\":\n",
    "            self.net_liquidation = float(value)\n",
    "            print(f\"âœ… NetLiquidation: {value} {currency}\")\n",
    "            self.received_nav.set()\n",
    "\n",
    "    def accountSummaryEnd(self, reqId):\n",
    "        print(\"âœ… Finished account summary.\")\n",
    "\n",
    "    def historicalData(self, reqId: int, bar: BarData):\n",
    "        self.fx_rate = bar.close\n",
    "        print(f\"âœ… FX EUR.USD Rate: {bar.close}\")\n",
    "        self.received_fx.set()\n",
    "\n",
    "    def historicalDataEnd(self, reqId: int, start: str, end: str):\n",
    "        print(\"âœ… Finished receiving FX data.\")\n",
    "\n",
    "def create_fx_contract(pair_str):\n",
    "    base, quote = pair_str.split(\".\")\n",
    "    contract = Contract()\n",
    "    contract.symbol = base\n",
    "    contract.secType = \"CASH\"\n",
    "    contract.exchange = \"IDEALPRO\"\n",
    "    contract.currency = quote\n",
    "    return contract\n",
    "\n",
    "# === Start IB connection ===\n",
    "app = IBapi()\n",
    "app.connect(\"127.0.0.1\", 7497, clientId=101)\n",
    "threading.Thread(target=app.run, daemon=True).start()\n",
    "time.sleep(1)\n",
    "\n",
    "# === Request NAV (EUR) ===\n",
    "app.reqAccountSummary(9001, \"All\", \"NetLiquidation\")\n",
    "if not app.received_nav.wait(timeout=10):\n",
    "    print(\"âŒ NAV not received within timeout.\")\n",
    "    exit()\n",
    "\n",
    "# === Request FX EUR.USD ===\n",
    "fx_contract = create_fx_contract(\"EUR.USD\")\n",
    "app.reqHistoricalData(\n",
    "    reqId=5001,\n",
    "    contract=fx_contract,\n",
    "    endDateTime=\"\",\n",
    "    durationStr=\"1 D\",\n",
    "    barSizeSetting=\"1 day\",\n",
    "    whatToShow=\"MIDPOINT\",\n",
    "    useRTH=0,\n",
    "    formatDate=2,\n",
    "    keepUpToDate=False,\n",
    "    chartOptions=[]\n",
    ")\n",
    "if not app.received_fx.wait(timeout=10):\n",
    "    print(\"âŒ FX rate not received within timeout.\")\n",
    "    exit()\n",
    "\n",
    "# === Compute NAV in USD ===\n",
    "nav_eur = app.net_liquidation\n",
    "fx = app.fx_rate\n",
    "nav_usd = nav_eur * fx\n",
    "today = pd.Timestamp.now(tz='UTC').strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "print(f\"\\nğŸ’° NAV in EUR: {nav_eur:,.2f}\")\n",
    "print(f\"ğŸ’± EUR.USD FX: {fx:.6f}\")\n",
    "print(f\"ğŸ’µ NAV in USD: {nav_usd:,.2f}\")\n",
    "\n",
    "# === Append to CSV file with drift ===\n",
    "out_path = \"ib_nav_log.csv\"\n",
    "new_row = {\n",
    "    \"Date\": today,\n",
    "    \"NAV_EUR\": nav_eur,\n",
    "    \"FX_EURUSD\": fx,\n",
    "    \"NAV_USD\": nav_usd,\n",
    "    \"NAV_USD_Drift_Pct\": None  # will compute next\n",
    "}\n",
    "\n",
    "if os.path.exists(out_path):\n",
    "    df = pd.read_csv(out_path)\n",
    "    if not df.empty:\n",
    "        previous_usd = df.iloc[-1][\"NAV_USD\"]\n",
    "        if previous_usd != 0:\n",
    "            drift = ((nav_usd - previous_usd) / previous_usd) * 100\n",
    "            new_row[\"NAV_USD_Drift_Pct\"] = round(drift, 4)\n",
    "    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "else:\n",
    "    df = pd.DataFrame([new_row])\n",
    "\n",
    "df.to_csv(out_path, index=False)\n",
    "print(f\"\\nâœ… NAV logged to {out_path}\")\n",
    "\n",
    "# === Print drift comparison ===\n",
    "if new_row[\"NAV_USD_Drift_Pct\"] is not None:\n",
    "    print(f\"\\nğŸ“ˆ NAV Drift vs Previous: {new_row['NAV_USD_Drift_Pct']:.2f}%\")\n",
    "else:\n",
    "    print(\"\\nâ„¹ï¸ No prior NAV to compare.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a95b662",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0099d4",
   "metadata": {},
   "source": [
    "Once the IB NAV is retrieved, the model NAV from the Zipline backtest is aligned with the live NAV reported by IB. The purpose is to measure deviations and check consistency between the backtest and the brokerage account.\n",
    "We begin with daily percentage returns from the backtest. Starting from $1,000,000 initial capital, the cumulative NAV is computed by compounding these returns. Only dates common to both datasets are kept, so weekends and non-trading days are excluded naturally.\n",
    "\n",
    "For each source we calculate day-over-day percentage changes:\n",
    "\n",
    "- Zipline_Model_Change = model NAV return\n",
    "- IB_Model_Change = actual account NAV return\n",
    "- Drift_Return_Pct = IB return â€“ Zipline return\n",
    "\n",
    "This drift shows whether live performance diverges from the model.\n",
    "\n",
    "The results are saved in nav_drift_comparison.csv, which can be used to flag outliers and monitor where differences between the model and live account accumulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac485a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === File Paths ===\n",
    "zipline_path = r\"C:\\Users\\Juanan\\Downloads\\CQF\\1.Futures_Trend_Following\\1.Zipline_Trades\\2.Deployment_Backtest\\returns_2025-08-15_to_2025-08-18_TF.csv\"\n",
    "ib_path = r\"C:\\Users\\Juanan\\Downloads\\CQF\\ib_nav_log.csv\"\n",
    "output_path = r\"C:\\Users\\Juanan\\Downloads\\CQF\\nav_drift_comparison.csv\"\n",
    "\n",
    "# === Load Zipline returns ===\n",
    "zip_df = pd.read_csv(zipline_path)\n",
    "zip_df[\"Date\"] = pd.to_datetime(zip_df.iloc[:, 0]).dt.date\n",
    "zip_df = zip_df.rename(columns={\"returns\": \"Zipline_Model_Change\"})\n",
    "zip_df = zip_df.sort_values(\"Date\")\n",
    "\n",
    "# Reconstruct NAV from initial capital\n",
    "initial_nav = 1_000_000\n",
    "zip_df[\"Zipline_Model_NAV\"] = initial_nav * (1 + zip_df[\"Zipline_Model_Change\"]).cumprod()\n",
    "\n",
    "# === Load IB NAV ===\n",
    "ib_df = pd.read_csv(ib_path)\n",
    "ib_df[\"Date\"] = pd.to_datetime(ib_df[\"Date\"]).dt.date\n",
    "ib_df = ib_df.rename(columns={\"NAV_USD\": \"IB_Model_NAV\"})\n",
    "\n",
    "# === Merge ===\n",
    "merged = pd.merge(zip_df[[\"Date\", \"Zipline_Model_NAV\"]], ib_df[[\"Date\", \"IB_Model_NAV\"]], on=\"Date\", how=\"inner\")\n",
    "\n",
    "# === Calculate changes directly from NAVs ===\n",
    "merged[\"Zipline_Model_Change\"] = merged[\"Zipline_Model_NAV\"].pct_change()\n",
    "merged[\"IB_Model_Change\"] = merged[\"IB_Model_NAV\"].pct_change()\n",
    "merged[\"Drift_Return_Pct\"] = merged[\"IB_Model_Change\"] - merged[\"Zipline_Model_Change\"]\n",
    "\n",
    "# === Convert percentage returns to 0.00% format ===\n",
    "merged[\"Zipline_Model_Change\"] = (merged[\"Zipline_Model_Change\"] * 100).round(4)\n",
    "merged[\"IB_Model_Change\"] = (merged[\"IB_Model_Change\"] * 100).round(4)\n",
    "merged[\"Drift_Return_Pct\"] = (merged[\"Drift_Return_Pct\"] * 100).round(4)\n",
    "\n",
    "# === Format NAV columns with commas ===\n",
    "merged[\"Zipline_Model_NAV\"] = merged[\"Zipline_Model_NAV\"].map(\"{:,.2f}\".format)\n",
    "merged[\"IB_Model_NAV\"] = merged[\"IB_Model_NAV\"].map(\"{:,.2f}\".format)\n",
    "\n",
    "# === Final output ===\n",
    "output = merged[[\n",
    "    \"Date\",\n",
    "    \"Zipline_Model_NAV\",\n",
    "    \"IB_Model_NAV\",\n",
    "    \"Zipline_Model_Change\",\n",
    "    \"IB_Model_Change\",\n",
    "    \"Drift_Return_Pct\"\n",
    "]]\n",
    "\n",
    "# === Save and display ===\n",
    "output.to_csv(output_path, index=False)\n",
    "display(output)\n",
    "print(f\"\\nâœ… Drift comparison saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a6cae0",
   "metadata": {},
   "source": [
    "Note that discrepancies are larger because the initial capital in IB is higher due to past operations in the demo account, and it cannot be reset in the personal account. For this reason, the visualization should be considered only as an example.\n",
    "\n",
    "This section then plots the daily return series from both the model and the IB account, together with the drift. The plot helps to spot deviations, execution errors, or data mismatches. A consistent strategy with proper execution should show small and infrequent drift, while systematic or growing divergence would point to potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17100721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Load comparison file ===\n",
    "comparison_path = r\"C:\\Users\\Juanan\\Downloads\\CQF\\nav_drift_comparison.csv\"\n",
    "df = pd.read_csv(comparison_path)\n",
    "\n",
    "# === Parse date ===\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "# === Convert percentage columns to numeric ===\n",
    "for col in [\"Zipline_Model_Change\", \"IB_Model_Change\", \"Drift_Return_Pct\"]:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# === Plot: Daily Return Comparison ===\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df[\"Date\"], df[\"Zipline_Model_Change\"], label=\"Zipline Model Change (%)\", marker=\"o\")\n",
    "plt.plot(df[\"Date\"], df[\"IB_Model_Change\"], label=\"IB Model Change (%)\", marker=\"o\")\n",
    "plt.plot(df[\"Date\"], df[\"Drift_Return_Pct\"], label=\"Drift Return (%)\", linestyle=\"--\", color=\"red\")\n",
    "plt.title(\"Daily Return Comparison\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Daily Return (%)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e0075b",
   "metadata": {},
   "source": [
    "Having aligned both NAV series and visualized the daily tracking error, the next step is to quantify how well the live portfolio matches the model over time. We compute performance metrics for both the Zipline model and the IB live account using the daily return series we already have. Metrics include cumulative and annualized return, volatility, Sharpe, Sortino, drawdowns, and ratios that describe the shape and asymmetry of returns. These are standard in assessing whether the live portfolio is tracking risk and return consistently.\n",
    "\n",
    "Finally, we regress IB returns on the model to extract alpha and beta â€” not for predictive use, but to assess if the live account has consistent exposure to the model and whether thereâ€™s any persistent drift or unexplained return component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315ee0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === Load data ===\n",
    "path = r\"C:\\Users\\Juanan\\Downloads\\CQF\\nav_drift_comparison.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "# === Convert returns to decimal ===\n",
    "df[\"Zipline_Model_Change\"] = pd.to_numeric(df[\"Zipline_Model_Change\"], errors=\"coerce\") / 100\n",
    "df[\"IB_Model_Change\"] = pd.to_numeric(df[\"IB_Model_Change\"], errors=\"coerce\") / 100\n",
    "\n",
    "# === Drop NaNs ===\n",
    "df = df.dropna(subset=[\"Zipline_Model_Change\", \"IB_Model_Change\"])\n",
    "\n",
    "# === Settings ===\n",
    "periods_per_year = 252\n",
    "\n",
    "def compute_metrics(returns: pd.Series, benchmark: pd.Series = None, name=\"\"):\n",
    "    cumulative = (1 + returns).prod() - 1\n",
    "    annualized = (1 + cumulative) ** (periods_per_year / len(returns)) - 1\n",
    "    volatility = returns.std() * np.sqrt(periods_per_year)\n",
    "    sharpe = annualized / volatility if volatility > 0 else np.nan\n",
    "    downside_std = returns[returns < 0].std() * np.sqrt(periods_per_year)\n",
    "    sortino = annualized / downside_std if downside_std > 0 else np.nan\n",
    "    drawdown = (1 + returns).cumprod().div((1 + returns).cumprod().cummax()) - 1\n",
    "    max_dd = drawdown.min()\n",
    "    calmar = annualized / abs(max_dd) if max_dd != 0 else np.nan\n",
    "    omega = returns[returns > 0].mean() / abs(returns[returns < 0].mean()) if not returns[returns < 0].empty else np.nan\n",
    "    skewness = returns.skew()\n",
    "    kurt = returns.kurt()\n",
    "    tail_ratio = returns[returns > 0].quantile(0.95) / abs(returns[returns < 0].quantile(0.05)) if not returns[returns < 0].empty else np.nan\n",
    "    var_95 = returns.quantile(0.05)\n",
    "\n",
    "    alpha = beta = np.nan\n",
    "    if benchmark is not None and not benchmark.empty:\n",
    "        cov = np.cov(returns, benchmark)[0][1]\n",
    "        var_bench = np.var(benchmark)\n",
    "        beta = cov / var_bench if var_bench != 0 else np.nan\n",
    "        alpha = (returns.mean() - beta * benchmark.mean()) * periods_per_year if not np.isnan(beta) else np.nan\n",
    "\n",
    "    return {\n",
    "        \"Name\": name,\n",
    "        \"Cumulative Return (%)\": round(cumulative * 100, 2),\n",
    "        \"Annualized Return (%)\": round(annualized * 100, 2),\n",
    "        \"Volatility (%)\": round(volatility * 100, 2),\n",
    "        \"Sharpe Ratio\": round(sharpe, 3),\n",
    "        \"Sortino Ratio\": round(sortino, 3),\n",
    "        \"Max Drawdown (%)\": round(max_dd * 100, 2),\n",
    "        \"Calmar Ratio\": round(calmar, 3),\n",
    "        \"Omega Ratio\": round(omega, 3),\n",
    "        \"Skewness\": round(skewness, 3),\n",
    "        \"Kurtosis\": round(kurt, 3),\n",
    "        \"Tail Ratio\": round(tail_ratio, 3),\n",
    "        \"Daily VaR (5%) (%)\": round(var_95 * 100, 2),\n",
    "        \"Alpha vs Zipline (%)\": round(alpha * 100, 2) if name == \"IB\" else None,\n",
    "        \"Beta vs Zipline\": round(beta, 4) if name == \"IB\" else None\n",
    "    }\n",
    "\n",
    "# === Compute metrics ===\n",
    "zip_metrics = compute_metrics(df[\"Zipline_Model_Change\"], name=\"Zipline\")\n",
    "ib_metrics = compute_metrics(df[\"IB_Model_Change\"], benchmark=df[\"Zipline_Model_Change\"], name=\"IB\")\n",
    "\n",
    "# === Combine ===\n",
    "metrics_df = pd.DataFrame([zip_metrics, ib_metrics])\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428f33fd",
   "metadata": {},
   "source": [
    "Right now we only have one interval to compare the Zipline NAV and the IB NAV. That makes the drift number and metrics basically meaningless on its own â€” it could be just noise from rounding, FX rates, or a timing mismatch. The only way to judge whether there is a systematic difference is to collect more data over time. Once we have a longer series, we can see whether the drift averages out or whether it compounds in one direction. Until then, itâ€™s just a single observation, not something we can act on.\n",
    "\n",
    "### 11.2. Position-Level Attribution â€“ Open Positions and Cumulative Returns\n",
    "\n",
    "Following the portfolio-level drift and performance consistency checks, we now move to a position-level analysis. The goal here is to identify which contracts are currently open as of the most recent trading date, trace back to their entry points, and compute the return each position has generated since entry. This gives us a clearer view of which exposures are contributing to current performance and where risk is currently concentrated.\n",
    "\n",
    "The script loads the full position history from the Zipline reloaded backtest, where each column represents a contract's USD value over time. After identifying the most recent trading day, we isolate contracts with non-zero positions â€” these are the trades still open.\n",
    "\n",
    "For each open contract, we locate the most recent day it transitioned from zero to a non-zero USD value, assuming that marks the entry point. We then compute the cumulative return as the percentage change in USD value from entry to the current date.\n",
    "\n",
    "The output includes:\n",
    "\n",
    "- Contract name\n",
    "\n",
    "- Entry date\n",
    "\n",
    "- Last date\n",
    "\n",
    "- Entry and current USD values\n",
    "\n",
    "- Cumulative return %\n",
    "\n",
    "Cash positions are excluded. The final table is sorted by return in descending order to highlight the top contributors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa3139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === Load positions file ===\n",
    "positions_file = r\"C:\\Users\\Juanan\\Downloads\\CQF\\1.Futures_Trend_Following\\1.Zipline_Trades\\2.Deployment_Backtest\\positions_2025-08-15_to_2025-08-18_TF.csv\"\n",
    "df = pd.read_csv(positions_file)\n",
    "\n",
    "# === Fix date column ===\n",
    "df[\"Date\"] = pd.to_datetime(df[\"index\"])\n",
    "df = df.drop(columns=[\"index\"])\n",
    "\n",
    "# === Identify last date (most recent) ===\n",
    "last_date = df[\"Date\"].max()\n",
    "last_positions = df[df[\"Date\"] == last_date].drop(columns=[\"Date\"])\n",
    "\n",
    "# === Find contracts with open positions on the last date (non-zero) ===\n",
    "open_contracts = last_positions.loc[:, (last_positions != 0).any(axis=0)].T\n",
    "open_contracts.columns = [\"Open_Position_USD\"]\n",
    "open_contracts[\"Contract\"] = open_contracts.index\n",
    "\n",
    "# === Initialize return tracking ===\n",
    "returns = []\n",
    "\n",
    "# === Loop through each open contract ===\n",
    "for contract in open_contracts[\"Contract\"]:\n",
    "    contract_series = df[[\"Date\", contract]].copy()\n",
    "    contract_series = contract_series[contract_series[contract] != 0]\n",
    "\n",
    "    entry_date = contract_series[\"Date\"].iloc[-1]\n",
    "    last_value = contract_series[contract_series[\"Date\"] == last_date][contract].values[0]\n",
    "    entry_value = contract_series[contract_series[\"Date\"] == entry_date][contract].values[0]\n",
    "\n",
    "    cumulative_return = ((last_value - entry_value) / abs(entry_value)) * 100 if entry_value != 0 else 0\n",
    "\n",
    "    returns.append({\n",
    "        \"Contract\": contract,\n",
    "        \"Entry_Date\": entry_date.date(),\n",
    "        \"Last_Date\": last_date.date(),\n",
    "        \"Entry_Value_USD\": entry_value,\n",
    "        \"Last_Value_USD\": last_value,\n",
    "        \"Cumulative_Return_%\": round(cumulative_return, 4)\n",
    "    })\n",
    "\n",
    "# === Create final DataFrame ===\n",
    "returns_df = pd.DataFrame(returns)\n",
    "\n",
    "# === Format values ===\n",
    "returns_df[\"Entry_Value_USD\"] = returns_df[\"Entry_Value_USD\"].map(lambda x: f\"{x:,.2f}\")\n",
    "returns_df[\"Last_Value_USD\"] = returns_df[\"Last_Value_USD\"].map(lambda x: f\"{x:,.2f}\")\n",
    "\n",
    "# === Optional: drop cash row ===\n",
    "returns_df = returns_df[returns_df[\"Contract\"].str.lower() != \"cash\"]\n",
    "\n",
    "# === Sort by return descending ===\n",
    "returns_df = returns_df.sort_values(by=\"Cumulative_Return_%\", ascending=False)\n",
    "\n",
    "# === Display full ===\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "display(returns_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faa7b23",
   "metadata": {},
   "source": [
    "Having reviewed open positions from the backtest, we now cross-check with the live account via the IB API. This ensures that reported exposures match what is actually held at the broker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a89912b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "from decimal import Decimal\n",
    "from ibapi.client import EClient\n",
    "from ibapi.wrapper import EWrapper\n",
    "from ibapi.contract import Contract\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "class IBapi(EWrapper, EClient):\n",
    "    def __init__(self):\n",
    "        EClient.__init__(self, self)\n",
    "        self.positions = []\n",
    "        self.done = threading.Event()\n",
    "\n",
    "    def position(self, account, contract: Contract, position, avgCost):\n",
    "        try:\n",
    "            if getattr(contract, \"secType\", None) == \"FUT\" and position != 0:\n",
    "                # Normalize numeric types\n",
    "                pos_dec = Decimal(str(position))\n",
    "                avg_dec = avgCost if isinstance(avgCost, Decimal) else Decimal(str(avgCost))\n",
    "                entry_val = pos_dec * avg_dec\n",
    "\n",
    "                self.positions.append({\n",
    "                    \"Contract\": f\"Future({getattr(contract, 'conId', 'NA')} [{getattr(contract, 'localSymbol', 'NA')}])\",\n",
    "                    \"Entry_Date\": \"N/A\",\n",
    "                    \"Last_Date\": datetime.today().strftime('%Y-%m-%d'),\n",
    "                    \"Entry_Value_USD\": float(entry_val),\n",
    "                    \"Last_Value_USD\": float(entry_val),\n",
    "                    \"Cumulative_Return_%\": 0.0\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ position() error for {getattr(contract,'localSymbol','?')}: {e}\", file=sys.stderr)\n",
    "\n",
    "    def positionEnd(self):\n",
    "        print(\"âœ… Finished receiving open positions.\")\n",
    "        self.done.set()\n",
    "\n",
    "# === Run IB API ===\n",
    "app = IBapi()\n",
    "app.connect(\"127.0.0.1\", 7497, clientId=102)\n",
    "thread = threading.Thread(target=app.run, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "# === Request positions ===\n",
    "time.sleep(1)\n",
    "app.reqPositions()\n",
    "if not app.done.wait(timeout=15):\n",
    "    print(\"âŒ Timeout waiting for position data.\")\n",
    "    app.disconnect()\n",
    "    sys.exit(1)\n",
    "\n",
    "# === Create DataFrame and export ===\n",
    "df = pd.DataFrame(app.positions)\n",
    "\n",
    "if df.empty:\n",
    "    print(\"â„¹ï¸ No open futures positions returned by IB.\")\n",
    "else:\n",
    "    for col in (\"Entry_Value_USD\", \"Last_Value_USD\", \"Cumulative_Return_%\"):\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").round(2)\n",
    "\n",
    "    # Save directly to Downloads\\CQF\n",
    "    output_path = r\"C:\\Users\\Juanan\\Downloads\\CQF\\ib_open_positions.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nâœ… Saved open futures positions to: {output_path}\\n\")\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "\n",
    "display(df)\n",
    "\n",
    "\n",
    "# === Clean up ===\n",
    "app.disconnect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13a5517",
   "metadata": {},
   "source": [
    "The difference between the two outputs arises because Interactive Brokers only reports positions that are already filled. Orders that are submitted but not executed â€” for example, because the market was closed or the limit price was not reached â€” do not appear in the position list. Once executed, the position will appear. If the market moves away from the limit price, the order remains pending until adjusted or canceled.\n",
    "This closes the loop between backtest and the live account. If the strategy logic is consistent and market structure unchanged, the two views should align over time. Any differences can then be measured directly instead of assumed.\n",
    "\n",
    "As a final note, full containerization of the execution pipeline will not be implemented at this stage. The system depends on Interactive Brokersâ€™ TWS/Gateway for API connectivity, which does not run reliably in headless environments like Docker without workarounds. In addition, Zipline Reloaded requires daily ingestion of the full Norgate data bundle, which makes container orchestration inefficient.\n",
    "\n",
    "Instead, the pipeline is modularized into scripts for ingestion, backtest, signal extraction, and order generation. These can be scheduled individually with a task scheduler or run manually after market close to maintain control and avoid unintended live order flow.\n",
    "\n",
    "For data storage, we are working directly with daily-exported bundles from Norgate. While this approach is simple and sufficient for development and execution, it has limitations: sensitivity to schema changes. A longer-term plan is to introduce a dedicated securities database (e.g., MySQL or PostgreSQL). This would decouple ingestion from model execution, simplify metadata handling (sector tags, tick sizes, contract specifications), and provide consistent access across tools. For now, given the limited scope of assets and data, the current approach remains acceptable and easy to inspect during the buildout phase."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
